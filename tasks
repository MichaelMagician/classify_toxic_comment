Preprocessor
    load embeding

transformer classifier
    encoder
        embedding
            position embedding
            token embedding
        transformer_block
            multihead-attention layer
            self-attention
                attention
                    * query, key, value matmul
                    * scale
                    * softmax
                activation with relu
        add & normolization                   
        forward feeed


    decoder    